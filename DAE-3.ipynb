{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c3389e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e84bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exp.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = []\n",
    "\n",
    "column_names = lines[0].strip().split(' ')\n",
    "\n",
    "# 在列名列表的开头插入额外的列名 \"gene\"\n",
    "column_names.insert(0, 'Gene')\n",
    "\n",
    "# 将处理后的列名添加到数据\n",
    "data.append(column_names)\n",
    "\n",
    "# 从第二行遍历每一行数据\n",
    "for line in lines[1:]:\n",
    "    # 去除行末的换行符并拆分数据\n",
    "    line_data = line.strip().split(' ')\n",
    "    # 将拆分后的数据添加到列表中\n",
    "    data.append(line_data)\n",
    "\n",
    "# 创建一个 Pandas DataFrame，并使用列名\n",
    "import pandas as pd\n",
    "\n",
    "# 将列表转换为 Pandas DataFrame\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "# 最后，将 DataFrame 保存为 CSV 文件\n",
    "df.to_csv('exp_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f38af882",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('methy.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = []\n",
    "\n",
    "column_names = lines[0].strip().split(' ')\n",
    "\n",
    "# 在列名列表的开头插入额外的列名 \"DNA\"\n",
    "column_names.insert(0, 'DNA')\n",
    "\n",
    "# 将处理后的列名添加到数据\n",
    "data.append(column_names)\n",
    "\n",
    "# 从第二行遍历每一行数据\n",
    "for line in lines[1:]:\n",
    "    # 去除行末的换行符并拆分数据\n",
    "    line_data = line.strip().split(' ')\n",
    "    # 将拆分后的数据添加到列表中\n",
    "    data.append(line_data)\n",
    "\n",
    "# 创建一个 Pandas DataFrame，并使用列名\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "# 最后，将 DataFrame 保存为 CSV 文件\n",
    "df.to_csv('methy_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "476b260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mirna.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = []\n",
    "\n",
    "column_names = lines[0].strip().split(' ')\n",
    "\n",
    "# 在列名列表的开头插入额外的列名 \"miRNA\"\n",
    "column_names.insert(0, 'miRNA')\n",
    "\n",
    "# 将处理后的列名添加到数据\n",
    "data.append(column_names)\n",
    "\n",
    "# 从第二行遍历每一行数据\n",
    "for line in lines[1:]:\n",
    "    # 去除行末的换行符并拆分数据\n",
    "    line_data = line.strip().split(' ')\n",
    "    # 将拆分后的数据添加到列表中\n",
    "    data.append(line_data)\n",
    "\n",
    "# 创建一个 Pandas DataFrame，并使用列名\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "# 最后，将 DataFrame 保存为 CSV 文件\n",
    "df.to_csv('mirna_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2abb084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = pd.read_csv('exp_file.csv')\n",
    "methy=pd.read_csv('methy_file.csv')\n",
    "mirna=pd.read_csv('mirna_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9844c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于将列名中的 \"-\" 替换为 \".\"\n",
    "def replace_dash_with_dot(column_name):\n",
    "    return column_name.replace('-', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "027e4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 rename() 方法替换列名\n",
    "methy.rename(columns=replace_dash_with_dot, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cbe9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_columns = set(exp.columns)\n",
    "methy_columns = set(methy.columns)\n",
    "mirna_columns = set(mirna.columns)\n",
    "# exp & methy & mirna相同相同column名字的列数 每一个column代表一名病人 也就是3个omics data记录的common病人数量\n",
    "common_columns_count = len(exp_columns & methy_columns & mirna_columns)\n",
    "common_columns_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787c962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = exp.columns.intersection(methy.columns).intersection(mirna.columns)\n",
    "# 选择这些列\n",
    "exp_common = exp[common_columns]\n",
    "methy_common = methy[common_columns]\n",
    "mirna_common = mirna[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "126d0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data = pd.concat([exp_common, methy_common, mirna_common], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693be070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "standard_scaler = StandardScaler()\n",
    "multi_data_standardized = standard_scaler.fit_transform(multi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ce3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26236, 291)\n"
     ]
    }
   ],
   "source": [
    "print(multi_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddfa069e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 26236)\n"
     ]
    }
   ],
   "source": [
    "#置换行和列，将数据的特征（列）和样本（行）重新排列\n",
    "tcga_input=np.transpose(multi_data_standardized)\n",
    "print(tcga_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e62f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 定义神经网络的隐藏层的大小（神经元的数量）的参数\n",
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 100\n",
    "n_hidden_3 = 50\n",
    "\n",
    "# 将 TensorFlow 切换回 graph execution 模式\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# 输入数据的特征数量\n",
    "n_input = tcga_input.shape[1]\n",
    "\n",
    "\n",
    "# 创建输入占位符，维度设置为 (None, n_input)\n",
    "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "# 神经网络的权重矩阵和偏置项\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random.normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n",
    "    'encoder_h3': tf.Variable(tf.random.normal([n_hidden_2, n_hidden_3])),\n",
    "    'decoder_h1': tf.Variable(tf.random.normal([n_hidden_3, n_hidden_2])),\n",
    "    'decoder_h2': tf.Variable(tf.random.normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h3': tf.Variable(tf.random.normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random.normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'encoder_b3': tf.Variable(tf.random.normal([n_hidden_3])),\n",
    "    'decoder_b1': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'decoder_b2': tf.Variable(tf.random.normal([n_hidden_1])),\n",
    "    'decoder_b3': tf.Variable(tf.random.normal([n_input])),\n",
    "}\n",
    "\n",
    "# 编码器函数---------如果是DAE后面要加biases['encoder_b1']，这是和你的autoencoder不一样的地方，你可以自己对比一下代码\n",
    "def encoder(x):\n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights['encoder_h3']), biases['encoder_b3']))\n",
    "    return layer_3\n",
    "\n",
    "# 解码器函数\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights['decoder_h3']), biases['decoder_b3']))\n",
    "    return layer_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d6ba2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 18:35:21.640946: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9496\n",
      "Epoch 100, Loss: 1.5310\n",
      "Epoch 200, Loss: 1.5316\n",
      "Epoch 300, Loss: 1.5328\n",
      "Epoch 400, Loss: 1.5334\n",
      "Epoch 500, Loss: 1.5331\n",
      "Epoch 600, Loss: 1.5340\n",
      "Epoch 700, Loss: 1.5341\n",
      "Epoch 800, Loss: 1.5344\n",
      "Epoch 900, Loss: 1.5344\n"
     ]
    }
   ],
   "source": [
    "# 定义 DAE 的输入和输出\n",
    "\n",
    "noise_factor = 0.1  # 引入的噪声因子\n",
    "\n",
    "# 使用 tf.random.normal 生成随机噪声，维度与 multi_data_standardized 相同\n",
    "noisy_data = tcga_input + noise_factor * tf.random.normal(shape=tf.shape(tcga_input), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "\n",
    "# 编码器和解码器的输出\n",
    "encoded = encoder(noisy_data)\n",
    "decoded = decoder(encoded)\n",
    "\n",
    "# 损失函数：均方差\n",
    "cost = tf.reduce_mean(tf.square(X - decoded))\n",
    "\n",
    "# 优化器\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 获取编码器和解码器的可训练变量\n",
    "encoder_trainable_variables = list(weights.values()) + list(biases.values())\n",
    "decoder_trainable_variables = list(weights.values()) + list(biases.values())\n",
    "\n",
    "# 将编码器和解码器的可训练变量合并为一个列表\n",
    "trainable_variables = encoder_trainable_variables + decoder_trainable_variables\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# 使用优化器最小化损失\n",
    "train_op = optimizer.minimize(cost, var_list=trainable_variables)\n",
    "\n",
    "# 初始化 TensorFlow 变量\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# 定义训练步骤\n",
    "def train_step():\n",
    "    _, c = sess.run([train_op, cost], feed_dict={X: noisy_data.eval()})\n",
    "    return c\n",
    "\n",
    "# 设置训练周期数量和显示步骤\n",
    "num_epochs = 1000\n",
    "display_step = 100\n",
    "\n",
    "# 运行训练步骤\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_step()\n",
    "        if epoch % display_step == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcc4c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Tanh_5:0\", shape=(291, 26236), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd320cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
